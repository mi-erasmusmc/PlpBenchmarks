
@article{zhang_benchmarking_2021,
	title = {Benchmarking feature selection methods with different prediction models on large-scale healthcare event data},
	volume = {1},
	issn = {2772-4859},
	url = {https://www.sciencedirect.com/science/article/pii/S2772485921000041},
	doi = {10.1016/j.tbench.2021.100004},
	abstract = {With the development of the Electronic Health Record (EHR) technique, vast volumes of digital clinical data are generated. Based on the data, many methods are developed to improve the performance of clinical predictions. Among those methods, Deep Neural Networks (DNN) have been proven outstanding with respect to accuracy by employing many patient instances and events (features). However, each patient-specific event requires time and money. Collecting too many features before making a decision is insufferable, especially for time-critical tasks such as mortality prediction. So it is essential to predict with high accuracy using as minimal clinical events as possible, which makes feature selection a critical question. This paper presents detailed benchmarking results of various feature selection methods, applying different classification and regression algorithms for clinical prediction tasks, including mortality prediction, length of stay prediction, and ICD-9 code group prediction. We use the publicly available dataset, Medical Information Mart for Intensive Care III (MIMIC-III), in our experiments. Our results show that Genetic Algorithm (GA) based methods perform well with only a few features and outperform others. Besides, for the mortality prediction task, the feature subset selected by GA for one classifier can also be used to others while achieving good performance.},
	number = {1},
	urldate = {2024-01-07},
	journal = {BenchCouncil Transactions on Benchmarks, Standards and Evaluations},
	author = {Zhang, Fan and Luo, Chunjie and Lan, Chuanxin and Zhan, Jianfeng},
	month = oct,
	year = {2021},
	keywords = {Deep neural networks, Feature selection, Genetic algorithm, Healthcare prediction},
	pages = {100004},
	file = {ScienceDirect Snapshot:files/5283/S2772485921000041.html:text/html},
}

@article{wu_time-series_2023,
	title = {Time-series benchmarks based on frequency features for fair comparative evaluation},
	volume = {35},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-023-08562-5},
	doi = {10.1007/s00521-023-08562-5},
	abstract = {Time-series prediction and imputation receive lots of attention in academic and industrial areas. Machine learning methods have been developed for specific time-series scenarios; however, it is difficult to evaluate the effectiveness of a certain method on other new cases. In the perspective of frequency features, a comprehensive benchmark for time-series prediction is designed for fair evaluation. A prediction problem generation process, composed of the finite impulse response filter-based approach and problem setting module, is adopted to generate the NCAA2022 dataset, which includes 16 prediction problems. To reduce the computational burden, the filter parameters matrix is divided into sub-matrices. The discrete Fourier transform is introduced to analyze the frequency distribution of transformed results. In addition, a baseline experiment further reflects the benchmarking capability of NCAA2022 dataset.},
	language = {en},
	number = {23},
	urldate = {2024-01-07},
	journal = {Neural Comput \& Applic},
	author = {Wu, Zhou and Jiang, Ruiqi},
	month = aug,
	year = {2023},
	keywords = {Evaluation dataset, Frequency domain, NCAA2022, Time-series prediction},
	pages = {17029--17041},
	file = {Full Text PDF:files/5285/Wu and Jiang - 2023 - Time-series benchmarks based on frequency features.pdf:application/pdf},
}

@article{breit_openbiolink_2020,
	title = {{OpenBioLink}: a benchmarking framework for large-scale biomedical link prediction},
	volume = {36},
	issn = {1367-4803},
	shorttitle = {{OpenBioLink}},
	url = {https://doi.org/10.1093/bioinformatics/btaa274},
	doi = {10.1093/bioinformatics/btaa274},
	abstract = {Recently, novel machine-learning algorithms have shown potential for predicting undiscovered links in biomedical knowledge networks. However, dedicated benchmarks for measuring algorithmic progress have not yet emerged. With OpenBioLink, we introduce a large-scale, high-quality and highly challenging biomedical link prediction benchmark to transparently and reproducibly evaluate such algorithms. Furthermore, we present preliminary baseline evaluation results.Source code and data are openly available at https://github.com/OpenBioLink/OpenBioLink.Supplementary data are available at Bioinformatics online.},
	number = {13},
	urldate = {2024-01-07},
	journal = {Bioinformatics},
	author = {Breit, Anna and Ott, Simon and Agibetov, Asan and Samwald, Matthias},
	month = jul,
	year = {2020},
	pages = {4097--4098},
	file = {Full Text PDF:files/5287/Breit et al. - 2020 - OpenBioLink a benchmarking framework for large-sc.pdf:application/pdf;Snapshot:files/5288/5825726.html:text/html},
}

@article{harutyunyan_multitask_2019,
	title = {Multitask learning and benchmarking with clinical time series data},
	volume = {6},
	copyright = {2019 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-019-0103-9},
	doi = {10.1038/s41597-019-0103-9},
	abstract = {Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.},
	language = {en},
	number = {1},
	urldate = {2024-01-07},
	journal = {Sci Data},
	author = {Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram},
	month = jun,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Databases, Disease-free survival, Machine learning},
	pages = {96},
	file = {Full Text PDF:files/5290/Harutyunyan et al. - 2019 - Multitask learning and benchmarking with clinical .pdf:application/pdf},
}

@article{dueben_challenges_2022,
	title = {Challenges and {Benchmark} {Datasets} for {Machine} {Learning} in the {Atmospheric} {Sciences}: {Definition}, {Status}, and {Outlook}},
	volume = {1},
	issn = {2769-7525},
	shorttitle = {Challenges and {Benchmark} {Datasets} for {Machine} {Learning} in the {Atmospheric} {Sciences}},
	url = {https://journals.ametsoc.org/view/journals/aies/1/3/AIES-D-21-0002.1.xml},
	doi = {10.1175/AIES-D-21-0002.1},
	abstract = {Benchmark datasets and benchmark problems have been a key aspect for the success of modern machine learning applications in many scientiﬁc domains. Consequently, an active discussion about benchmarks for applications of machine learning has also started in the atmospheric sciences. Such benchmarks allow for the comparison of machine learning tools and approaches in a quantitative way and enable a separation of concerns for domain and machine learning scientists. However, a clear deﬁnition of benchmark datasets for weather and climate applications is missing with the result that many domain scientists are confused. In this paper, we equip the domain of atmospheric sciences with a recipe for how to build proper benchmark datasets, a (nonexclusive) list of domain-speciﬁc challenges for machine learning is presented, and it is elaborated where and what benchmark datasets will be needed to tackle these challenges. We hope that the creation of benchmark datasets will help the machine learning efforts in atmospheric sciences to be more coherent, and, at the same time, target the efforts of machine learning scientists and experts of high-performance computing to the most imminent challenges in atmospheric sciences. We focus on benchmarks for atmospheric sciences (weather, climate, and air-quality applications). However, many aspects of this paper will also hold for other aspects of the Earth system sciences or are at least transferable.},
	language = {en},
	number = {3},
	urldate = {2024-01-07},
	journal = {Artificial Intelligence for the Earth Systems},
	author = {Dueben, Peter D. and Schultz, Martin G. and Chantry, Matthew and Gagne, David John and Hall, David Matthew and McGovern, Amy},
	month = jul,
	year = {2022},
	pages = {e210002},
	file = {Dueben et al. - 2022 - Challenges and Benchmark Datasets for Machine Lear.pdf:files/5291/Dueben et al. - 2022 - Challenges and Benchmark Datasets for Machine Lear.pdf:application/pdf},
}

@article{xie_benchmarking_2022,
	title = {Benchmarking emergency department prediction models with machine learning and public electronic health records},
	volume = {9},
	copyright = {2022 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01782-9},
	doi = {10.1038/s41597-022-01782-9},
	abstract = {The demand for emergency department (ED) services is increasing across the globe, particularly during the current COVID-19 pandemic. Clinical triage and risk assessment have become increasingly challenging due to the shortage of medical resources and the strain on hospital infrastructure caused by the pandemic. As a result of the widespread use of electronic health records (EHRs), we now have access to a vast amount of clinical data, which allows us to develop prediction models and decision support systems to address these challenges. To date, there is no widely accepted clinical prediction benchmark related to the ED based on large-scale public EHRs. An open-source benchmark data platform would streamline research workflows by eliminating cumbersome data preprocessing, and facilitate comparisons among different studies and methodologies. Based on the Medical Information Mart for Intensive Care IV Emergency Department (MIMIC-IV-ED) database, we created a benchmark dataset and proposed three clinical prediction benchmarks. This study provides future researchers with insights, suggestions, and protocols for managing data and developing predictive tools for emergency care.},
	language = {en},
	number = {1},
	urldate = {2024-01-07},
	journal = {Sci Data},
	author = {Xie, Feng and Zhou, Jun and Lee, Jin Wee and Tan, Mingrui and Li, Siqi and Rajnthern, Logasan S/O and Chee, Marcel Lucas and Chakraborty, Bibhas and Wong, An-Kwok Ian and Dagan, Alon and Ong, Marcus Eng Hock and Gao, Fei and Liu, Nan},
	month = oct,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Health care, Medical research},
	pages = {658},
	file = {Full Text PDF:files/5294/Xie et al. - 2022 - Benchmarking emergency department prediction model.pdf:application/pdf},
}

@article{rajpurkar_ai_2022,
	title = {{AI} in health and medicine},
	volume = {28},
	copyright = {2022 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-021-01614-0},
	doi = {10.1038/s41591-021-01614-0},
	abstract = {Artificial intelligence (AI) is poised to broadly reshape medicine, potentially improving the experiences of both clinicians and patients. We discuss key findings from a 2-year weekly effort to track and share key developments in medical AI. We cover prospective studies and advances in medical image analysis, which have reduced the gap between research and deployment. We also address several promising avenues for novel medical AI research, including non-image data sources, unconventional problem formulations and human–AI collaboration. Finally, we consider serious technical and ethical challenges in issues spanning from data scarcity to racial bias. As these challenges are addressed, AI’s potential may be realized, making healthcare more accurate, efficient and accessible for patients worldwide.},
	language = {en},
	number = {1},
	urldate = {2024-01-07},
	journal = {Nat Med},
	author = {Rajpurkar, Pranav and Chen, Emma and Banerjee, Oishi and Topol, Eric J.},
	month = jan,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Medical research},
	pages = {31--38},
	file = {Full Text PDF:files/5296/Rajpurkar et al. - 2022 - AI in health and medicine.pdf:application/pdf},
}

@article{purushotham_benchmarking_2018,
	title = {Benchmarking deep learning models on large healthcare datasets},
	volume = {83},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046418300716},
	doi = {10.1016/j.jbi.2018.04.007},
	abstract = {Deep learning models (aka Deep Neural Networks) have revolutionized many fields including computer vision, natural language processing, speech recognition, and is being increasingly used in clinical healthcare applications. However, few works exist which have benchmarked the performance of the deep learning models with respect to the state-of-the-art machine learning models and prognostic scoring systems on publicly available healthcare datasets. In this paper, we present the benchmarking results for several clinical prediction tasks such as mortality prediction, length of stay prediction, and ICD-9 code group prediction using Deep Learning models, ensemble of machine learning models (Super Learner algorithm), SAPS II and SOFA scores. We used the Medical Information Mart for Intensive Care III (MIMIC-III) (v1.4) publicly available dataset, which includes all patients admitted to an ICU at the Beth Israel Deaconess Medical Center from 2001 to 2012, for the benchmarking tasks. Our results show that deep learning models consistently outperform all the other approaches especially when the ‘raw’ clinical time series data is used as input features to the models.},
	urldate = {2024-01-07},
	journal = {Journal of Biomedical Informatics},
	author = {Purushotham, Sanjay and Meng, Chuizheng and Che, Zhengping and Liu, Yan},
	month = jul,
	year = {2018},
	keywords = {Deep learning models, ICD-9 code group prediction, Length of stay, Mortality prediction, Super learner algorithm},
	pages = {112--134},
	file = {ScienceDirect Snapshot:files/5300/S1532046418300716.html:text/html},
}

@inproceedings{wang_mimic-extract_2020,
	address = {New York, NY, USA},
	series = {{CHIL} '20},
	title = {{MIMIC}-{Extract}: a data extraction, preprocessing, and representation pipeline for {MIMIC}-{III}},
	isbn = {978-1-4503-7046-2},
	shorttitle = {{MIMIC}-{Extract}},
	url = {https://dl.acm.org/doi/10.1145/3368555.3384469},
	doi = {10.1145/3368555.3384469},
	abstract = {Machine learning for healthcare researchers face challenges to progress and reproducibility due to a lack of standardized processing frameworks for public datasets. We present MIMIC-Extract, an open source pipeline for transforming the raw electronic health record (EHR) data of critical care patients from the publicly-available MIMIC-III database into data structures that are directly usable in common time-series prediction pipelines. MIMIC-Extract addresses three challenges in making complex EHR data accessible to the broader machine learning community. First, MIMIC-Extract transforms raw vital sign and laboratory measurements into usable hourly time series, performing essential steps such as unit conversion, outlier handling, and aggregation of semantically similar features to reduce missingness and improve robustness. Second, MIMIC-Extract extracts and makes prediction of clinically-relevant targets possible, including outcomes such as mortality and length-of-stay as well as comprehensive hourly intervention signals for ventilators, vasopressors, and fluid therapies. Finally, the pipeline emphasizes reproducibility and extensibility to future research questions. We demonstrate the pipeline's effectiveness by developing several benchmark tasks for outcome and intervention forecasting and assessing the performance of competitive models.},
	urldate = {2024-01-07},
	booktitle = {Proceedings of the {ACM} {Conference} on {Health}, {Inference}, and {Learning}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Shirly and McDermott, Matthew B. A. and Chauhan, Geeticka and Ghassemi, Marzyeh and Hughes, Michael C. and Naumann, Tristan},
	month = apr,
	year = {2020},
	keywords = {Healthcare, Machine learning, MIMIC-III, Reproducibility, Time series data},
	pages = {222--235},
	file = {Full Text PDF:files/5299/Wang et al. - 2020 - MIMIC-Extract a data extraction, preprocessing, a.pdf:application/pdf},
}

@article{seneviratne_bridging_2020,
	title = {Bridging the implementation gap of machine learning in healthcare},
	volume = {6},
	copyright = {© Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
	issn = {2055-8074, 2055-642X},
	url = {https://innovations.bmj.com/content/6/2/45},
	doi = {10.1136/bmjinnov-2019-000359},
	abstract = {Applications of machine learning on clinical data are now attaining levels of performance that match or exceed human clinicians.1–3 Fields involving image interpretation—radiology, pathology and dermatology—have led the charge due to the power of convolutional neural networks, the existence of standard data formats and large data repositories. We have also seen powerful diagnostic and predictive algorithms built using a range of other data, including electronic health records (EHR), -omics, monitoring signals, insurance claims and patient-generated data.4 The looming extinction of doctors has captured the public imagination, with editorials such as ‘The AI Doctor Will See You Now’.5 The prevailing view among experts is more balanced: that doctors who use artificial intelligence (AI) will replace those who do not.6

Amid such inflated expectations, the elephant in the room is the implementation gap of machine learning in healthcare.7 8 Very few of these algorithms ever make it to the bedside; and even the most technology-literate academic medical centres are not routinely using AI in clinical workflows. A recent systematic review of deep learning applications using EHR data highlighted the need to focus on the last mile of implementation: ‘for direct clinical impact, deployment and automation of deep learning models must be considered’.9 The typical life-cycle of an algorithm remains: train on historical data, publish a good receiver-operator curve and then collect dust in the ‘model graveyard’.

This begs the question: if model performance is so promising, why is there such a chasm between development and deployment? In order …},
	language = {en},
	number = {2},
	urldate = {2024-01-07},
	journal = {BMJ Innovations},
	author = {Seneviratne, Martin G. and Shah, Nigam H. and Chu, Larry},
	month = apr,
	year = {2020},
	note = {Publisher: BMJ Specialist Journals
Section: Commentary},
	keywords = {artificial intelligence, clinical decision support, deployment, implementation, machine learning},
	file = {Full Text PDF:files/5302/Seneviratne et al. - 2020 - Bridging the implementation gap of machine learnin.pdf:application/pdf},
}

@article{franklin_opportunistic_2011,
	series = {Biomedical {Complexity} and {Error}},
	title = {Opportunistic decision making and complexity in emergency care},
	volume = {44},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046411000657},
	doi = {10.1016/j.jbi.2011.04.001},
	abstract = {In critical care environments such as the emergency department (ED), many activities and decisions are not planned. In this study, we developed a new methodology for systematically studying what are these unplanned activities and decisions. This methodology expands the traditional naturalistic decision making (NDM) frameworks by explicitly identifying the role of environmental factors in decision making. We focused on decisions made by ED physicians as they transitioned between tasks. Through ethnographic data collection, we developed a taxonomy of decision types. The empirical data provide important insight to the complexity of the ED environment by highlighting adaptive behavior in this intricate milieu. Our results show that half of decisions in the ED we studied are not planned, rather decisions are opportunistic decision (34\%) or influenced by interruptions or distractions (21\%). What impacts these unplanned decisions have on the quality, safety, and efficiency in the ED environment are important research topics for future investigation.},
	number = {3},
	urldate = {2024-01-07},
	journal = {Journal of Biomedical Informatics},
	author = {Franklin, Amy and Liu, Ying and Li, Zhe and Nguyen, Vickie and Johnson, Todd R. and Robinson, David and Okafor, Nnaemeka and King, Brent and Patel, Vimla L. and Zhang, Jiajie},
	month = jun,
	year = {2011},
	keywords = {Across-task decisions, Complexity, Emergency department, Methodology, Opportunistic decision making, Opportunistic planning, Taxonomy of decision types},
	pages = {469--476},
	file = {ScienceDirect Snapshot:files/5304/S1532046411000657.html:text/html},
}

@article{hargrove_bench--bedside_2005,
	title = {Bench-to-bedside review: {Outcome} predictions for critically ill patients in the emergency department},
	volume = {9},
	issn = {1364-8535},
	shorttitle = {Bench-to-bedside review},
	url = {https://doi.org/10.1186/cc3518},
	doi = {10.1186/cc3518},
	abstract = {The escalating number of emergency department (ED) visits, length of stay, and hospital overcrowding have been associated with an increasing number of critically ill patients cared for in the ED. Existing physiologic scoring systems have traditionally been used for outcome prediction, clinical research, quality of care analysis, and benchmarking in the intensive care unit (ICU) environment. However, there is limited experience with scoring systems in the ED, while early and aggressive intervention in critically ill patients in the ED is becoming increasingly important. Development and implementation of physiologic scoring systems specific to this setting is potentially useful in the early recognition and prognostication of illness severity. A few existing ICU physiologic scoring systems have been applied in the ED, with some success. Other ED specific scoring systems have been developed for various applications: recognition of patients at risk for infection; prediction of mortality after critical care transport; prediction of in-hospital mortality after admission; assessment of prehospital therapeutic efficacy; screening for severe acute respiratory syndrome; and prediction of pediatric hospital admission. Further efforts at developing unique physiologic assessment methodologies for use in the ED will improve quality of patient care, aid in resource allocation, improve prognostic accuracy, and objectively measure the impact of early intervention in the ED.},
	number = {4},
	urldate = {2024-01-07},
	journal = {Critical Care},
	author = {Hargrove, Jenny and Nguyen, H. Bryant},
	month = apr,
	year = {2005},
	keywords = {Emergency Department, Glasgow Coma Scale, Injury Severity Score, Severe Acute Respiratory Syndrome, Systemic Inflammatory Response Syndrome},
	pages = {376},
	file = {Full Text PDF:files/5306/Hargrove and Nguyen - 2005 - Bench-to-bedside review Outcome predictions for c.pdf:application/pdf;Snapshot:files/5307/cc3518.html:text/html},
}
